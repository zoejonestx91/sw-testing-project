\newcommand{\ct}{\mbox{\textsc{CoverageTool}}}
\newcommand{\mt}{\mbox{\textsc{MetricsTool}}}

\section{Problem Statement}

Our first problem is to create a tool capable of instrumenting Java programs using Java Agent to collect coverage information for JUnit tests. We will refer to this tool as \ct{}.

Our second problem is to create a tool that will statically compute a set of metrics using ASM. We will refer to this tool as \mt{}.

We would like to be able to offer our project as a basis for future extension to tackle research problems with regards to software metrics. 

\section{Technique Discussion}

\subsection{Instrumentation and Analysis}

To disassemble and analyze target projects, we will use ASM and Java Agent.

\subsection{Metrics of Interest}

There are several metrics that we were interested in  collecting. We had planned to cover all metrics listed under the JHawk method metrics listing~\cite{jhawkmetrics}, as well as some of the class metrics, and a couple of extra method metrics. Unfortunately, some method level metrics proved to not be possible using just bytecode. Below, we speak to a selection of the metrics that proved especially challenging, interesting, or just impossible.

\subsubsection{Comments}

Unfortunately, we are not able to do any metrics involving comments, as this is not something that is accessible from bytecode. 

\subsubsection{Cyclomatic Complexity}

This metric is constructed from the attributes of the program's control flow graph. Since the project only demands statement-level coverage at minimum, additional coverage analysis (in the form of branch-level coverage) may be necessary in order to determine this metric.

\subsubsection{Classification}

We were interested in also implementing simple heuristics to allow method classification for some level of statistical analysis. For example, identifying getters, setters, and constructors. We propose to also allow these heuristics to be extended by end users.

\section{Implementation Plan}
\ct{} was written to be ran under Maven, for easy integration into existing projects. It will be able to output its results to the terminal or a file. If a project has an appropriately modified \texttt{pom.xml} file, then \ct{} can be run by using \texttt{mvn test}. A feature that we had been considering is a script that will attempt to patch the relevant \texttt{pom.xml} file to include \ct{}.

\mt{} will at the least have a command line tool, to ease its usage in scripts. We had considered a GUI frontend, but we had concerns about the practicality of this in the given time period, and whether this would be suitable to the needs of the researcher.

Both tools are written in Java, so as to be able to take advantage of Java's reflection tools to disassemble and instrument projects. They do not share code between them, but experience gained from \ct{} was used to engineer \mt{}.

\section{Experimental Plan}

\subsection{Source Project Selection}

We attempted to select our projects with the following attributes in mind.

\begin{itemize}
\item \textbf{Size} - We are required to select projects with at least 1,000 Lines of Code. To help validate the tool, we would want to have at least two that are near this limit, to simplify initial testing.
\item \textbf{Test Coverage} - We would like to have plenty of test coverage available on each project. In order to satisfy this attribute, we likely will start with an initial set of 15 projects under test, and narrow our scope to the ten with the best coverage.
\item \textbf{Maturity} - To give us a good sample set, we would prefer mature, long running projects.
\item \textbf{Type} - Generally, libraries and APIs will be easier to test than apps with GUIs. In particular, for the sake of automated testing, we would prefer projects that can run headless.
\item \textbf{Variety} - A selection of projects with different purposes increases our ability to test the robustness of our tools. Since projects with different aims are more likely to adopt different approaches to solving their problems, they are more likely to expose erroneous behavior in our tools.
\end{itemize}

The following is our current list of test candidates:

\begin{itemize}
	\item \textbf{apache/commons-codec} - An encoding library.
    \item \textbf{apache/commons-lang} - A set of utilities for Java's lang package, or considered common enough to be included in lang.
    \item \textbf{apache/commons-io} - A set of IO utilities.
    \item \textbf{Bukkit/Bukkit} - A video game server API for Minecraft.
    \item \textbf{evernote/evernote-sdk-java} - The cloud API for a note-taking application.
    \item \textbf{docker-java/docker-java} - A software container API.
    \item \textbf{junit-team/junit4} - A testing framework.
    \item \textbf{RoaringBitmap/RoaringBitmap} - A data structure library
    \item \textbf{wg/scrypt} - A cryptographic library.
    \item \textbf{xerial/sqlite-jdbc} - A library for accessing SQLite databases in Java.
   \item \textbf{sk89q/WorldGuard} - A Bukkit plugin.
\end{itemize}

\subsection{Validation}

To validate our work, we plan to compare our results to currently available tools such as SourceMeter. 

\section{CoverageTool}

\subsection{Design}

\subsubsection{Architecture}

Early in the design phase it became apparent that the design of \ct{} would include three primary modules: a Java Agent meant to setup and coordinate the bytecode transformer, the bytecode transformer itself, and the JUnit RunListener. While it may have been possible to implement \ct{} without the agent, it proved to be the best way to accept configuration arguments in the context of Maven. In particular, the agent allows us to specify in the \texttt{pom.xml} which packages or classes should be monitored for coverage information. As a result, the agent module is rather small, as its only purpose is to serve as an initializer and argument handler. The other two modules are indispensable and provide the main functionality.

In order to fulfill the coverage collection requirement, we implement a bytecode transformation module that can be configured to target specific packages and classes. Since bytecode transformations are more difficult to implement, we anticipated that direct reliance of the transformations on features of other modules would be unwieldy. As a result, most of our iterations on this module featured an approach to instrumentation that simply instructed targeted bytecode to call out to some static handler method, which would then implement any interaction with our data storage submodule.

Finally, since the requirements of the \ct{} project demand that coverage information be collected with respect to individual tests, an extension of the JUnit RunListener class was necessary. This modules serves as the collection hub for various data. Its purposes include setting up coverage data structures on a per-test basis, managing configuration options provided by the agent, and writing the coverage information as output at the end of a test run. The test listener module uses as a submodule our storage mechanism, which assists in managing coverage information. This particular submodule has seen a great deal of iteration.

\subsubsection{Runtime Re-Instrumentation}

Our initial plans for the transformation module were significantly more complex and would likely have resulted in a more sophisticated architecture. We were concerned from the beginning that instrumenting large projects would incur a large overhead if many of the instrumented methods were not relevant to the tests being run. Indeed, between the \texttt{commons-dbutils} and \texttt{joda-time} projects, we found that some source files had upwards of 2000 lines of code. If a test run included only a fraction of the methods in a large source file, many methods would be instrumented that would never be tested. To this end, we set out to limit our transformations such that only those units that were necessary would be instrumented.

Rather than perform some sort of static analysis or depend on feedback from dynamic analysis (which may itself rely on a method of detecting which methods were covered by a test run), we opted to intercept calls to undiscovered methods and instrument them at runtime. Of course, this would require not only that we implement instrumentation to detect when user code called undiscovered user code, but that we instrument the user tests themselves in order to detect calls to targeted user code.

This strategy, while interesting, proved difficult to implement during the early stages. During this time we were informed that the overhead of the instrumentation process is actually rather low compared to the typical overhead introduced by the instrumentation itself. In light of this, we decided to change our approach. A comparison between this strategy and the more straightforward approach that we adopted is left for future work.

\subsubsection{Single Pass}

Discarding the need for method-level instrumentation granularity renders the process rather straightforward. The \texttt{java.lang.instrument} package, by default, will only instrument classes as they are picked up by the class loader. This guarantees at least class-level instrumentation granularity and excludes classes that, even if targeted, would not ever have their code run. As a result, there is no need to instrument the user's test cases or perform any sort of detection of uninstrumented code, as the framework handles this for us.

Our transformation module consists of a few extensions to the \texttt{instrument} package's classes. We implement a custom transformer class that distinguishes between targeted packages and classes based on fully-qualified names. We also implement a very simple custom class visitor that stores class ID information derived from our storage submodule. This ID is passed to a custom method visitor that detects when lines are visited. For the sake of simplicity the handling of visited lines was such that the class ID and line number would be provided to a static handler method with access to our storage mechanism, but our latest iterations instead feature direct bytecode access to the storage mechanism.

\subsection{Iterations and Intermediate Results}

After its first full implementation, \ct{} went through another four iterations to improve its performance, which was encountering a very serious bottleneck in the storage submodule. While there were perhaps two more potential approaches, the four iterations \ct{} went through proved sufficient to reduce its overhead to about 80\% for the \texttt{joda-time} project, one of two projects we initially tested it on.

While our initial implementation used an ordered tree to store coverage information, this caused \texttt{joda-time}, which had previously had test times of about 15 seconds, to take 20 minutes. Our research indicated that boolean arrays, the BitSet class, or storage utilizing bitwise operations may be good options, so we chose to pursue some of these.

It is important to note that we store coverage information on a per-test basis so that it may be sorted before being output. Since the coverage is performed for every test and each test has unique coverage data, we essentially used a two-dimensional array for each test, with one index referring to a class ID and the other referring to a line number. The array entry would then indicate whether or not the line had been covered by the test in question.

\subsubsection{Static Boolean Arrays}

Our readings online suggested that the use of arrays of boolean values was perhaps the fastest way of storing the information we needed. In Java, booleans don't have a strictly defined size; they typically take the size of an integer (32 bits), which reflects their implementation in the JVM. Boolean arrays, on the other hand, confine each boolean to a single byte. Of course, this means each boolean still consumes 7 more bits than necessary to store its information.

After finding that our original tree-based storage method did not work well, this was our first iteration on the storage submodule. We found conservatively large sizes for the coverage arrays based on the anticipated maximum number of classes and line numbers. Memory issues were expected to some extent, as we store the coverage information until the end of the entire test run. For 4000 tests over 500 classes each expected to have a maximum of 2500 lines, 4.66GB of memory should be necessary for the boolean arrays alone. Unfortunately, even allocating 8GB of memory to a 64-bit JVM did not prevent it from crashing.

\subsubsection{HashMaps}

After the failure of the boolean approach above, we chose to return roughly to our original architecture and make some modifications that would take advantage of new information. Using the Java HashMap class, we restructured our original approach such that a dynamic array of test records could rely on our original tree assumptions. Rather than use the TreeSet class for organizing class records, we simply used HashMaps from class IDs to class records, which then stored HashSets of integer values representing line numbers covered. This finally brought the \texttt{joda-time} test duration down to about 1.5 minutes. Of course, this was still not enough to be competitive, so we chose to iterate further.

% If this was in fact done, I forgot to ever commit it. The performance must not have been good enough. I did not document its impact.
% \subsubsection{Dynamic Boolean Arrays}

% To reduce the memory consumption of the previous approach, we opted not to rely on estimates for the number of classes or line numbers to bound the size of the arrays. Instead, we used the standard ArrayList class in place of static arrays. With the problem of memory eliminated, the approach worked just fine. The time required to test \texttt{joda-time} was reduced to around ??, a vast improvement over the 20 minutes required by the tree-based approach. Still, additional changes would be necessary to make it competitive with other implementations.

\subsubsection{Dynamic Bit Arrays}

While it was not clear whether the space overhead of using booleans would be causing any performance losses, and while it seemed that HashSets ought to be efficient enough to meet our requirements, we decided that the next approach was still worth trying for as long as improvements were known to be possible. Since we still had the BitSet structure and bitwise operations to choose between, a judgment had to be made between them. According to our research, the BitSet implementation approaches the efficiency of manual bitwise operations on extremely large data sets, implying that bitwise operations should generally be preferable, especially for a problem as simple as ours.

To this end, we iterated on the previous designs by replacing the boolean entries of the static arrays with integers contained in dynamic arrays. Using bitwise operations, each integer could then store a true or false flag for 32 different line numbers in a single class. Using dynamic arrays meant that we would be able to work independently of any assumption about the number of classes to be instrumented or the number of lines in each class.

This approach was successful in making our tool more competitive, as the test time for \texttt{joda-time} was reduced to 1.25 minutes. Still, this was not good enough to even make an entry for the second round.

\subsubsection{Static Bit Arrays}

With the shift to bitwise operations proving successful in reducing execution time, it became appropriate to reconsider statically-sized arrays. Storing coverage information at bit-level granularity (as opposed to byte-level) reduces memory consumption by a factor of 8, assuming the boolean arrays do indeed store each boolean using 8 bits. With this in mind, we reimplemented the assumptions about class numbers and line numbers in order to allocate statically-sized arrays of integers. The result was such that no changes needed to be made to the JVM's memory arguments and execution succeeded. Moreover, the \texttt{joda-time} project was successfully tested in around 20 seconds, placing our tool firmly in the lead of the others in the competition.

The only other change at play was the removal of an attempted optimization to detect classes that were not covered by a test. The purpose of this optimization had been to speed up the time to output the coverage information, but it was found that removing this functionality reduced test time to 1 minute prior to the above array implementation. The 40-second speedup that followed was a surprising result considering how little was changed.

\subsection{Evaluation}

Unfortunately, due to a mixup of deadlines we were unable to complete our evaluation for \ct{} or \mt{}. We focused primarily on doing an analysis on \mt{}, and this came at the cost of any evaluation of \ct{}.

\subsection{Future Work}

We overlooked one potential optimization for the static array approaches that may have allowed the boolean strategy to work and may have sped up the bit strategy further. Rather than allocating the nested array corresponding to line numbers \textit{when} a class finally gets covered, we allocate them all at once presuming each class will be covered. Furthermore, because the number of lines a class contains can be discovered by an ASM class visitor, it is possible to size these arrays appropriately to each class. This strategy is far more reasonable than our current approach.

\section{MetricsTool}


\subsection{Design and Architecture}

The authors had high hopes for the original design and scope of \mt{}, and Ms. Jones has intentions of attempting to expand the architecture and scope of this tool. However, under scheduling pressure and unexpected obstacles, the authors had to reduce the scope to something that just works. The submitted version of \mt{} is referred to as \textbf{RC1}, and the planned version (and hopeful next release) is referred to as \textbf{R2}. RC1 uses workarounds to ensure that it could meet it's deadline. Some of the planned subsystems are included, but are in a partially completed state, and thus are disabled. We document our original intentions, the design's evolution, and our implementation in RC1. As of this writing \mt{} represents 3788 source lines of Java code.

\subsection{Planned Design and Architecture}
As our imagined primary use case is intended for research purposes, we are emphasizing extensibility and versatility over performance. So, \mt{} uses an plugin-based architecture. To simplify development and to provide an example for end users, our default metrics are defined as plugins themselves. For our list of default metrics for initial release, please see the listing in Table 1. While \mt{} takes lessons and some code from \ct{}, they are not directly compatible, as we do not need performance at the cost of everything else. The plugins are configured by a suite of Java annotations that are interpreted on load.

Plugins are exposed to the ASM API, and use pretty much the same set of methods as standard ASM visitors. We will not document the Visitor architecture here, as it is well documented in the case of ASM.

The collected metrics are stored by TableStore in a set of Tables, which are divided by the type of artifact. The cells in the tables are referenced by artifact identifier and metric identity.

\mt{} operates in multiple execution phases.
\begin{enumerate}
\item \textbf{Plugin Load} - Plugins are loaded from \texttt{./plugins/}, and are scanned for configuration annotations.
\item \textbf{Preinstrumentation} - Plugins have access to code before instrumentation. This is helpful for static analysis.
\item \textbf{Instrumentation} - The target code is instrumented by the Instrumenter.
\item \textbf{Test} - Tests are permitted to execute. 
\item \textbf{Postprocessing} - After tests are completed, plugins are allowed to do postprocessing, which is intended for plugins to calculate derived values.
\item \textbf{Output} - The contents of TableStore are written to disk for external analysis.
\end{enumerate}

\mt{} can collect metrics at the following levels:

\begin{itemize}
\item \textbf{package} - Packages can contain other packages, classes, and interfaces.
\item \textbf{class/interface} - As interfaces work on the same level as classes, and can be used as a type due to polymorphism and inheritance, we treat interfaces as a special case of classes.
\item \textbf{method} - Methods are contained by a particular class. 
\item \textbf{block} - Blocks are a sequence of code that execute without a disruption of execution flow.
\item \textbf{statement}
\end{itemize}

Instrumentation is performed using that same strategy as in \ct{}. Plugins are able to perform work before and after the instrumentation. When instrumented code is called, it connects to a handler in the Scheduler, which will then fire callbacks registered by the plugins. This will notify relevant plugins of the artifact, and allow them to analyze it.

\mt{} was also originally designed to be capable of formatting output for delimited files, JSON, XML, or serialization.

\subsection{Implementation of RC1}

RC1 is designed to execute during the compile goal of Maven, and outputs a comma delimited table to standard output. At this juncture, we unfortunately do not use our planned Scheduler, Table subsystem, or annotation configuration subsystem.

% \begin{table*}[]
% \centering
% \caption{Method Level Metrics in \mt{} RC1}
% \label{my-label}
% \begin{tabular}{|l|l|}
% \hline
% \textbf{Metric} & \textbf{Description} \\ \hline
% Name                &                      \\ \hline
% Cyclomatic Complexity                &  Quantifies the complexity of code, and thus how many test cases are necessary for all paths coverage.~\cite{mccabe}                    \\ \hline
% Modifiers                &  The access modifiers on a particular field. eg, public, static, volatile \\ \hline

% \end{tabular}
% \end{table*}

\subsection{Discussion}

Compared to tools like JHawk, \mt{} has some weaknesses. Since we were aiming to implement all of the JHawk methods that were feasible, a major goal for the evaluation phase was to duplicate exactly the results for the metrics that we can collect. Unfortunately, while we can approach certain metrics using our own strategies, it is impossible to replicate some of the results produced by JHawk. We suspect that it almost certainly analyzes source code directly too.

For more notes on individual metric implementation details, see the file \texttt{metrics.md} under the \mt{} project documentation directory.

\subsubsection{Tracking Control Flow}

We had the opportunity to build a control flow graph while traversing the bytecode using ASM. While we ultimately chose not to do this (instead favoring a more dynamic approach) for such metrics as the cyclomatic complexity metric, some of the problems we encoutered would have been the same. Tracking the control flow becomes difficult unless every possible type of jump has been accouted for. It was easy to identify the major bytecode features associated with basic loops and conditional statements, but we did not have time to consider all possible cases. Among those that remain for future work are switch statements and the throwing of exceptions. Exceptions are particularly tricky as it is difficult to analyze exactly which statements can be the source of exceptions without extensive analysis.

\subsubsection{Discovering Variable Declarations}

The Java bytecode contains information on a per-method basis that documents the \textit{used} local variables. Unfortunately, source code may contain unused local variables that nevertheless occupy space on the stack. We can actually infer that these variables exists, but it is not always possible to reason about how many of them there are as different types in Java have different sizes on the stack. We expect that our numbers on this metric will be lower than those produced by JHawk, which has the advantage of performing analysis on the source code.

\subsubsection{Cast Operations}

Casts between different types of objects are of two general types: upcasts and downcasts. Upcasts may be implicit or explicit, as an object can be readily assigned to a name of a parent type, so performing an explicit cast is optional. This is not the case for downcasts, as the JVM must check at run time whether or not the cast is valid. This manifests as an operation in bytecode that can be used to track explicit downcasts.

Of course, since this bytecode operation is limited to downcasts, it becomes significantly more difficult to determine where upcasts are taking place. As a result of this, we only track downcasts.

\subsubsection{Counting Operators and Operands}

The Halstead metrics are defined in terms of the number of unique operators and operands, as well as the total number of operators and operands. While one might expect the definitions of operator and operand to be restricted to the operations in Java that have dedicated syntax (such as \texttt{+}, \texttt{*}, or even the ternary operator \texttt{?:}), JHawk is inclusive of other pieces of syntax when determining these values. For example, method modifiers (public, static, etc.) affect the Halstead length, which is the sum of the total number of operators with the total number of operands.

This property does not in itself make it impossible to replicate JHawk's results. Initially we sought to reverse-engineer the code constructs that had effects on the Halstead metrics. Unfortunately, we discovered that some source code features that affect JHawk's metrics cannot be analyzed using bytecode alone, as \mt{} seeks to do. An example of such problematic code can be seen in Listing~\ref{lst:identicalmethods}.

\lstset{language=Java,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    commentstyle=\color{green},
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    basicstyle=\ttfamily
}
\begin{lstlisting}[frame=single,caption=Methods with identical bytecode,label=lst:identicalmethods]
public void m5() {
    B.m2();
}
public void m8() {
    B.m2();
    return;
}
\end{lstlisting}

These two methods compile to identical bytecode and hence cannot be distinguished between by ASM for the purposes of the Halstead metrics. On the other hand, since JHawk analyzes the source and considers a return statement to be an operator, it considers the two methods to have different values for the Halstead metrics.

We do not consider operators to have types. This is to say that we treat the addition operator as not having distinct variations to handle doubles versus integers.

Determining the number of unique operands proved the greatest challenge among the metrics. While it is easier to keep count of operands as they are used, the Halstead metrics need to know how many unique operands are used. This is problematic since bytecode does not represent each operation or operand in such a way that all the components of the operation are local to each other; other operations may take place in-between the must be tracked using some sort of stack emulation. This is exactly the approach we have taken, but it has some severe limitations as we have barely scratched the surface of the huge number of considerations that need to be made.

\subsection{Evaluation}
We evaluated RC1 against the Starter Edition of JHawk6. We chose to use this for comparison as the technology employed is similar, and as we were able to dump it's analysis of an entire project to a CSV file. This is also well used in research, according to JHawk's developer~\cite{jhawk_academic_papers}. This feature is unfortunately not available in the free version. We wrote a Python script to automate the comparison between the two CSV files.

For our purposes, we are considering JHawk6 to be the canonical implementation for our metrics (Though JHawk6 crashed on \texttt{commons-lang} - behavior that we do not attempt to replicate). Unfortunately, due to a scheduling mixup, we were not able to complete our analysis in time for publication. Nevertheless, we have made the collected data available on our project's repository.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \begin{table}[]
% \centering
% \caption{Error Rates for Method Metrics}
% \label{error}

% \begin{tabular}{@{}ll@{}}
% Metric                 & Error Rate \\ \hline
% Cyclomatic Complexity  & \textbf{}  \\
% Variable Declarations  & \textbf{}  \\
% Variable References    & \textbf{}  \\
% Max Depth of Nesting   &            \\
% Halstead Length        &            \\
% Halstead Vocabulary    &            \\
% Halstead Volume        &            \\
% Halstead Difficulty    &            \\
% Halstead Effort        &            \\
% Halstead Bugs          &            \\
% Total Depth of Nesting &            \\
% Number of Casts        &            \\
% Number of Loops        &           
% \end{tabular}
% \end{table}

\subsection{Future Work and Possible Applications}
This section includes both features under consideration and ideas for research that \mt{} could enable.

\subsubsection{Multithreading}
Currently, \mt{} is single-threaded. However, the design is meant to permit multithreading in the future without the need to modify plugins. 

\subsubsection{Storage Enhancements}
At this juncture, RC1 of \mt{} does not use the Table subsystem. The planned implementation had a fairly naive implementation for the Table subsystem that is not particularly efficient, and does not implement type-checking. In later iterations, we would hope to have more efficient mechanisms for storage.

\subsubsection{Support for Dynamic Instrumentation}
Currently, \mt{} does not have any plugins for instumentation or dynamic analysis. We had hoped to be able to implement this feature, but as of RC1, we did not find enough time to finish or test.

\subsubsection{Version Control Integration}
A possible future direction of \mt{} would be awareness and integration of version control systems into analysis capabilities. This would be helpful for being able to analyze the history of a project over a history of commits.

\subsubsection{Support for Aspect-Oriented Programmming}

At current, \mt{} is not configured to support Aspect Oriented Programming (AOP). Conceptually, Aspects are pieces of code that can wrap around other pieces of code, and are an effective means of dealing with cross cutting concerns. As what \mt{} would see is "weaved" into the class files, it would be unaware of how aspects influenced the bytecode present.

\section{GitHub Repository}

Our project, as well as instructions for it's usage, can be accessed at the following URL: \url{https://github.com/zoejonestx91/sw-testing-project}.